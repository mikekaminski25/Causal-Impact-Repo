---
title: "Causal-Impact"
author: "Mike Kaminski"
date: "2023-03-08"
output: html_document
---
# Initial Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lirbraries
```{r message = FALSE, warning = FALSE}
library(zoo)
library(lares)
library(yfR)
library(readr)
library(tidyr)
library(dplyr)
library(CausalImpact)
library(TTR)
```

# Stock Prices for Baseline
## Gets a list of all the tickers in the S&P500
```{r warning=FALSE}
# set.seed(1234)
# 
# n_tickers <- 500
# df_sp500 <- yf_index_composition("SP500")
# rnd_tickers <- base::sample(df_sp500$ticker, n_tickers)

```

##Extracts historical prices from yahoo finance - only run once, import stock prices from .csv
```{r message = FALSE, warning = FALSE}
# df_yf <- yf_get(tickers = rnd_tickers,
#                 first_date = '2020-01-02',
#                 last_date = Sys.Date())
# head(df_yf)
```

##Removes unnecessary columns and adds a 5, 10, and 20 day moving average.  These will come into play later in the code
```{r}
# df_stock_prices_ma <- df_yf %>%
#   select(ticker, ref_date, price_close) %>%
#   group_by(ticker) %>%
#   mutate(mov_avg5 = SMA(price_close, n =5)) %>%
#   mutate(mov_avg10 = SMA(price_close, n =10)) %>%
#   mutate(mov_avg20 = SMA(price_close, n =20))%>%
#   mutate(ref_date = as.Date(ref_date)) %>%
#   mutate(price_close = as.numeric(price_close)) %>%
#   ungroup()

```

##writes to a .csv and saves
```{r, include = FALSE}
# write.csv(df_stock_prices_ma, 'stocks0308.csv')
```

```{r}
df_stock_prices_ma <- read.csv(file.choose())
```

##Stocks are only priced on days the markets are open, so a sequence of dates has been created to account for the days when there wasn't a closing price.  Dates are right joined by the sequence of dates by ticker. Any day the market is closed means that the price_close value is zero
```{r}
df_date_list <- df_stock_prices_ma %>%
  group_by(ticker) %>%
  complete(ref_date = seq.Date(as.Date('2019-12-29'), Sys.Date(), by="day"))
```

## Replaces NAs with 10dma.  5-day or 20-day moving average can be used as well. Sales data is from Feb 1 2020, so filtering for that 
```{r}
df_stock_prices_ma_clean <- df_date_list %>%
  group_by(ticker) %>%
  fill(mov_avg10, .direction = "downup")  %>%
  mutate(price_close = ifelse(is.na(price_close), mov_avg10, price_close))  %>%
  select(c(ticker,ref_date,price_close)) %>%
  filter(ref_date >= '2020-02-01') %>% # first day of sales data
  mutate(price_close = round(price_close,2))%>%
  ungroup()
```

### Alternatively, a df is created based just on the 5-day moving average of the share price - 1 week
```{r}
df_stock_prices_5dma <- df_date_list %>%
  group_by(ticker) %>%
  fill(mov_avg5, .direction = "downup")  %>%
  select(c(ticker,ref_date,mov_avg5)) %>%
  filter(ref_date >= '2020-02-01') %>% #most recent sales data
  mutate(mov_avg5 = round(mov_avg5,2)) %>%
  select(ref_date, ticker, mov_avg5) %>%
  as.data.frame()
```

# Sales Data
## Import Data
```{r}
df_sales <- read.csv(file.choose())
```

## Converts sales_df date to appropriate date forma and creates a 7days moving average for sales
```{r}
df_sales$Date <- as.Date(df_sales$Date, format = "%m/%d/%Y")
df_sales_7dma <- df_sales %>%
  mutate(mov_avg7 = round(SMA(Sales, n =7),2))
```

## merges  daily sales and the daily stocks data
```{r}
df_final_daily <- merge(df_stock_prices_ma_clean, df_sales, by.x='ref_date',by.y='Date', all.x=TRUE) %>% 
  pivot_wider(names_from = ticker, values_from = price_close) %>%
  filter(Sales >=0) %>%
  as.data.frame()

```

## merges the 7dma sales and the daily stocks data
```{r}
df_final_7dma_daily <- merge(df_stock_prices_ma_clean, df_sales_7dma, by.x='ref_date',by.y='Date', all.x=TRUE) %>% 
  select(-c(Sales)) %>%
  pivot_wider(names_from = ticker, values_from = price_close) %>%
  filter(mov_avg7 >=0) %>%
  as.data.frame()
```

## merges the 7dma sales and 5dma stocks data
```{r}
df_final_7dma_5dma <- merge(df_stock_prices_5dma, df_sales_7dma, by.x='ref_date',by.y='Date', all.x=TRUE) %>% 
  select(-c(Sales)) %>%
  pivot_wider(names_from = ticker, values_from = mov_avg5) %>%
  filter(mov_avg7 >=0)%>%
  as.data.frame()
```

## In order to create a synthetic control group aka a baseline, we want to see which securities are most correlated to the number of sales.  Actual daily values are used initially, but log values as well as 7dma for sales and 5dma for stocks will be reviewed
```{r}
m <- df_final_daily[,-1]
a <- corr_var(m,
              Sales,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a, cex = .5,cex.axis=.5)
```
#### Correlations don't look too strong, the most correlated is MOH with a corr of -0.54.  Will try with log values of sales

## Log values of daily sales and stock prices
```{r}
m_log <- log(df_final_daily[,-1])
a_log <- corr_var(m_log,
              Sales,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_log, cex = .5,cex.axis=.5)
```
#### These results are actually worse than before.  There might be too much variablity in the daily sales data, so willcheck against the 7dma and the daily prices

## 7dma Sales and daily stock prices
```{r}
m_7dma <- df_final_7dma_daily[,-1]
a_7dma <- corr_var(m_7dma,
              mov_avg7,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_7dma, cex = .5,cex.axis=.5)
```
#### These results look a lot better.  The top 25 range from -.783 to -.727.

## 7dma sales and 5dma stock prices
```{r}
m_7dma_5dma <- df_final_7dma_5dma[,-1]
a_7dma_5dma <- corr_var(m_7dma_5dma,
              mov_avg7,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_7dma_5dma, cex = .5,cex.axis=.5)
```
#### 7dma and 5dma are better, ranging from -0.819 to -0.775

####Look at the top 100 for 7dma and 5dma
```{r}
m_7dma_5dma <- df_final_7dma_5dma[,-1]
a_7dma_5dma <- corr_var(m_7dma_5dma,
              mov_avg7,
              top = 100,
              #max_pvalue = 0.05
              )
# plot(a_7dma, cex = .5,cex.axis=.5) #the plot looks messy, but the values are included below
a_7dma_5dma$data$corr
a_7dma_5dma$data$variables
```
#### All hundred are above -0.70

## log of the 7dma sales and 5dma stock prices
```{r}
m_7dma_5dma_log <- log(df_final_7dma_5dma[,-1])
a_7dma_5dma_log <- corr_var(m_7dma_5dma_log,
              mov_avg7,
              top = 100,
              #max_pvalue = 0.05
              )
plot(a_7dma_5dma_log, cex = .5,cex.axis=.5) #the plot looks messy, but the values are included below
cor_log_7 <- a_7dma_5dma_log$data$corr
stock_log_7 <-a_7dma_5dma_log$data$variables
log_7 <- data.frame(col1 = stock_log_7,col2 =cor_log_7) #creates a dataframe to review
head(log_7)
which(abs(a_7dma_5dma_log$data$corr) >= (0.695))

```
```{r}
# mean of each
mean(abs(a_7dma_5dma$data$corr))
mean(abs(a_7dma_5dma_log$data$corr))
```
#### The log of 7dma and 5dma have a larger average correlation than without log, so these varibles will be used.  Just need to remember to take reverse log out of the final values

# Modeling

## Need to establish the pre and post periods.  These periods can be shortened as we tune the model if needed. Start date will be 03-01-2020 since that's the first full month of data.  The intervention (price increase) occurred on Jan 15th 2023. A variable called time points is created for the Causal Impact model
```{r}
pre.start <- "2020-03-01" #start of pre period
pre.end <- "2023-01-14" #end of pre period
post.start <- "2023-01-15" #intervention start
post.end <- "2023-02-27" #end of data/intervention end

pre.period <- as.Date(c(pre.start, pre.end))
post.period <- as.Date(c(post.start,post.end))

df_final_7dma_5dma_1 <- df_final_7dma_5dma %>% filter(ref_date >= pre.start) %>% select(-c(ref_date)) %>% log()
time.points <- seq.Date(as.Date(pre.start), as.Date(post.end), by =1)
```

### Other variables are created for the model as well.  The first 50 most correlated stocks will be used initially.  The top 100 were all fairly high, so it might be worth exploring all 100 or even more and also using less. 
```{r}
baseline_tickers <- a_7dma_5dma_log$data$variables[1:50] #takes the first 50 stocks from the correlation created earlier.

df_model <- as_tibble(df_final_7dma_5dma_1[,c('mov_avg7', baseline_tickers)]) # creates a tibble for the model

baseline <- zoo(df_model, order.by = time.points) #creates a time series for stocks and time.points

```

# Seasonality Tests

##no seasonality
```{r}
ptm <- proc.time() #starts the clock
impact_none <- CausalImpact(baseline, 
                            pre.period, 
                            post.period,
                            alpha = 0.05,
                            model.args = list(
                            niter = 10000
                            # prior.level.sd = 0.01, #default is 0.01
                            # nseasons = 12, #default is 1
                            # season.duration = 30, #default is 1
                            # dynamic.regression = FALSE, #default is FALSE
                            # max.flips = 10000 #default is -1
                           )
                        )

summary(impact_none)
# impact_none$model$model.args

proc.time() - ptm #stops the clock
```

## Daily Seasonality
```{r}
ptm <- proc.time() #starts the clock

impact_daily <- CausalImpact(baseline, 
                            pre.period, 
                            post.period,
                            alpha = 0.05,
                            model.args = list(
                            niter = 10000,
                            # prior.level.sd = 0.01, #default is 0.01
                            nseasons = 7, #default is 1
                            season.duration = 1 #default is 1
                            # dynamic.regression = FALSE, #default is FALSE
                            # max.flips = 10000 #default is -1
                           )
                        )

summary(impact_daily)
# impact_none$model$model.args

proc.time() - ptm #stops the clock
```

## Weekly seasonality
```{r}
ptm <- proc.time() #starts the clock

impact_weekly <- CausalImpact(baseline, 
                            pre.period, 
                            post.period,
                            alpha = 0.05,
                            model.args = list(
                            niter = 10000,
                            # prior.level.sd = 0.01, #default is 0.01
                            nseasons = 52, #default is 1
                            season.duration = 7 #default is 1
                            # dynamic.regression = FALSE, #default is FALSE
                            # max.flips = 10000 #default is -1
                           )
                        )

summary(impact_weekly)
# impact_none$model$model.args

proc.time() - ptm #stops the clock

```

## Monthly Seasonality
```{r}
ptm <- proc.time() #starts the clock

impact_monthly <- CausalImpact(baseline, 
                            pre.period, 
                            post.period,
                            alpha = 0.05,
                            model.args = list(
                            niter = 10000,
                            # prior.level.sd = 0.01, #default is 0.01
                            nseasons = 12, #default is 1
                            season.duration = 30 #default is 1
                            # dynamic.regression = FALSE, #default is FALSE
                            # max.flips = 10000 #default is -1
                           )
                        )

summary(impact_monthly)
# impact_none$model$model.args

proc.time() - ptm #stops the clock
```

## Quarterly Seasonality
```{r}
ptm <- proc.time() #starts the clock

impact_quarterly <- CausalImpact(baseline, 
                            pre.period, 
                            post.period,
                            alpha = 0.05,
                            model.args = list(
                            niter = 10000,
                            # prior.level.sd = 0.01, #default is 0.01
                            nseasons = 4, #default is 1
                            season.duration = 91 #default is 1
                            # dynamic.regression = FALSE, #default is FALSE
                            # max.flips = 10000 #default is -1
                           )
                        )

summary(impact_quarterly)
```

## shows all results.  The default model produces the best results, but daily is close as well.  None of them are significant
```{r}
init_results <- data.frame(none = impact_none$summary$p[1],
                           daily = impact_daily$summary$p[1],
                           weekly = impact_weekly$summary$p[1],
                           monthly = impact_monthly$summary$p[1],
                           quarterly = impact_quarterly$summary$p[1])
init_results

```

# Function to hypertune parameters
## Creats a grid of various hyperparameters.  Will try daily seasonality first
```{r}
#default is the first value
param_grid = list(niter = c(10000), # default is 1000
                  standardize.data = c(TRUE), #default is true
                  prior.level.sd = c(0.01,0.1), # default is 0.01
                  nseasons = c(1), #default is 1
                  season.duration = c(1), #default is 1
                  dynamic.regression = c(FALSE), #default is false
                  max.flips = c(-1,10,25,50) #default is -1
                 )

# Generate all combinations of parameters
params = expand.grid(param_grid)
params
```

```{r include=FALSE}
#creates a test parameter grid
n <- 2
test_param <- params[c(sample(1:nrow(params), n, replace = FALSE)),]
test_param
```

## Setup for function
```{r}

#creates an empty list to store p values of each model
p = c()

ptm <- proc.time() #starts the clock
for(i in 1:nrow(params)){
# for(i in 1:nrow(test_param)){

  # Fit a model using one parameter combination
  m = CausalImpact(baseline,
                   pre.period, 
                   post.period,
                   #alpha = 0.1, # can adjust alpha if necessary
                   model.args = list(
                     niter = params[i,1],
                     standardize.data = params[i,2],
                     prior.level.sd = params[i,3],
                     nseasons = params[i,4],
                     season.duration = params[i,5], 
                     dynamic.regression = params[i,6],
                     max.flips = params[i,7]
                     )
                   )

  # Model performance
  df_p <- m$summary$p[1]
  # Save model performance metrics
  p[i] = df_p
  print(proc.time() - ptm)[3]

}
 proc.time() - ptm #stops the clock
```

## Results of Modeling
```{r}
results <- data.frame(params,p) %>% arrange(p)
results
```
#### none of the results are significant, so I'll try to shorten the pre-period to see if better results are acquired, lowering the iterations so it moves faster.  It could be that there was no significant increase on sales when there was a price increase, but the assumption is that sales would have gone noticeably down



## Adjust the start dates
```{r}
param_grid = list(niter = c(1000), 
                  standardize.data = c(TRUE),
                  prior.level.sd = c(0.01, 0.1),
                  nseasons = c(1),
                  season.duration = c(1),
                  dynamic.regression = c(FALSE),
                  max.flips = c(-1,10,25,50),
                  pre.start = as.Date(c("2021-01-14","2022-01-14","2022-07-14"))
                 )

# Generate all combinations of parameters
params = expand.grid(param_grid)
params
```

```{r}

#creates an empty list to store p values of each model
p = c()

ptm <- proc.time() #starts the clock

#pre.start <- defined below
pre.end <- "2023-01-14" #end of pre period
post.start <- "2023-01-15" #intervention start
post.end <- "2023-02-27" #end of data/intervention end


for(i in 1:nrow(params)){
# for(i in 1:nrow(test_param)){

  # uses a different start date
  pre.period <- as.Date(c(params[i,8], pre.end))
  post.period <- as.Date(c(post.start,post.end))

  #adjust the time points based on the start date
  df_final_7dma_5dma <- df_final_7dma_5dma %>% filter(ref_date >= params[i,8])
  time.points <- seq.Date(as.Date(params[i,8]), as.Date(post.end), by =1)
  baseline <- zoo(df_model, order.by = time.points)
    
  
  
  # Fit a model using one parameter combination
  m = CausalImpact(baseline,
                   pre.period, 
                   post.period,
                   #alpha = 0.1, # can adjust alpha if necessary
                   model.args = list(
                     niter = params[i,1],
                     standardize.data = params[i,2],
                     prior.level.sd = params[i,3],
                     #nseasons = params[i,4],
                     #season.duration = params[i,5], 
                     dynamic.regression = params[i,6],
                     max.flips = params[i,7]
                     )
                   )

  # Model performance
  df_p <- m$summary$p[1]
  # Save model performance metrics
  p[i] = df_p
  print(proc.time() - ptm)[3]

}
 proc.time() - ptm #stops the clock
```

```{r}
results <- data.frame(params,p) %>% arrange(p)
results
```
# Only 1000 iterations were used so that it would run faster. A start date of 2022-01-14 appears to give us more significant results.  Additionally, a prior level sd of 0.01 also appears to be better than 0.10

##Final Model

```{r}
ptm <- proc.time() #starts the clock

pre.start <- "2022-01-14"  #start of pre period
pre.end <- "2023-01-14" #end of pre period
post.start <- "2023-01-15" #intervention start
post.end <- "2023-02-27" #end of data/intervention end

time.points <- time.points <- seq.Date(as.Date(pre.start), as.Date(post.end), by =1)
df_model <-
baseline <- zoo(df_model, order.by = time.points)

pre.period <- as.Date(c(pre.start, pre.end))
post.period <- as.Date(c(post.start,post.end))

impact <- CausalImpact(baseline, 
                       pre.period, 
                       post.period,
                       alpha = 0.05,
                       model.args = list(
                         niter = 250000,
                         prior.level.sd = 0.01, #default is 0.01
                         nseasons = 1, #default is 1
                         season.duration = 1, #default is 1
                         #dynamic.regression = FALSE, #default is FALSE
                         max.flips = 50 #default is -1                         
                          )
                       )
time <- proc.time() - ptm #stops the clock
time[3]/60
```

```{r}
summary(impact)
plot(impact)
```

```{r}
summary(impact,"report")
```

Test the model by changing the pre and post dates.  In theory, there shoulnd't be any effect since the price increase hadn't occurred

```{r}
pre.start <- "2022-01-14"  #start of pre period
pre.end <- "2022-12-01" #end of pre period
post.start <- "2022-12-02" #intervention start
post.end <- "2023-01-14" #end of data/intervention end
pre.period <- as.Date(c(pre.start, pre.end))
post.period <- as.Date(c(post.start,post.end))

df_final_7dma_5dma_back_test <- df_final_7dma_5dma %>% 
  filter(ref_date >= as.Date(pre.start) & ref_date <= as.Date(post.end))

time.points <- df_final_7dma_5dma_back_test[,1]

```

```{r}
baseline_tickers <- a_7dma_5dma_log$data$variables[1:50] #takes the first 50 stocks from the df

m_7dma_5dma_log_back_test <- log(df_final_7dma_5dma_back_test[,-1])

df_model_back_test <- as_tibble(m_7dma_5dma_log_back_test[,c('mov_avg7', baseline_tickers)]) #creates a tibble for the model

baseline <- zoo(df_model_back_test, order.by = time.points) #creates a time series for stocks and time.points
```


```{r}
ptm <- proc.time() #starts the clock

impact_back_test <- CausalImpact(baseline, 
                       pre.period, 
                       post.period,
                       alpha = 0.05,
                       model.args = list(
                         niter = 100000,
                         prior.level.sd = 0.01, #default is 0.01
                         nseasons = 7, #default is 1
                         season.duration = 1, #default is 1
                         #dynamic.regression = FALSE, #default is FALSE
                         max.flips = -1 #default is -1                         
                          )
                       )
time <- proc.time() - ptm #stops the clock
time[3]/60
```
```{r}
summary(impact_back_test)
plot(impact_back_test)
```

```{r}
summary(impact_back_test,"report")
```

```{r}
plot(impact$model$bsts.model, "coefficients")
```
```{r}
options(scipen = 999) 
a <-impact$model$bsts.model$coefficients
df_model_coeffs <- a %>%
  colMeans() %>%
  as.data.frame() %>%
  arrange(desc(abs(.)))

df_model_coeffs
```

reverses the zoo, remove NAs, selects columns, reverse logs, filters, finds difference
```{r}
df_series_results <- fortify(impact$series) %>% 
  na.omit() %>% 
  select(c(1,2,4),) %>%
  mutate_at(vars(2:3), exp) %>%
  rename(Date = Index) %>%
  filter(Date >= "2023-01-15") %>%
  mutate(week.num = week(Date)) %>%
  mutate(week.day = weekdays(Date)) %>%
  filter(week.day == 'Saturday') %>%
  mutate(point.effect = response - point.pred) %>%
  
df_series_results
```

plots the results
```{r}
ggplot(df_series_results, aes(x = Date)) +
  geom_line(aes(y = response, color = "Actual")) +
  geom_line(aes(y = point.pred, color = "Counterfactual")) +
  scale_color_manual(values = c("Actual" = "blue", "Counterfactual" = "red")) +
  labs(title = "Actual and Counterfactual 7dma on Each Saturday",
       x = "Date",
       y = "7dma Sales",
       color = "") +
  scale_x_date(date_labels = "%b %d", breaks = df_series_results$Date)


```


