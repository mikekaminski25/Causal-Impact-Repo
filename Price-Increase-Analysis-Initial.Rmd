---
title: "Price-Increase-Initial"
author: "Mike Kaminski"
date: "2023-03-13"
output: html_document
---
# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE, warning = FALSE}
library(zoo)
library(lares)
library(yfR)
library(readr)
library(tidyr)
library(dplyr)
library(CausalImpact)
library(TTR)
```

# Stock Prices for Baseline

Gets a list of all the tickers in the S&P500
```{r warning=FALSE,include=FALSE}
# set.seed(1234)
# 
# n_tickers <- 500
# df_sp500 <- yf_index_composition("SP500")
# rnd_tickers <- base::sample(df_sp500$ticker, n_tickers)

```

Extracts historical prices from yahoo finance
```{r message = FALSE, warning = FALSE, include=FALSE}
# this was run once for the analysis, but can be run again if needed

# df_yf <- yf_get(tickers = rnd_tickers,
#                 first_date = '2020-01-02',
#                 last_date = Sys.Date())
# head(df_yf)
```

Creates Moving Averages
```{r,include=FALSE}
#Removes unnecessary columns and adds a 5, 10, and 20 day moving average.  These will come into play later in the code

# df_stock_prices_ma <- df_yf %>%
#   select(ticker, ref_date, price_close) %>%
#   group_by(ticker) %>%
#   mutate(mov_avg5 = SMA(price_close, n =5)) %>%
#   mutate(mov_avg10 = SMA(price_close, n =10)) %>%
#   mutate(mov_avg20 = SMA(price_close, n =20))%>%
#   mutate(ref_date = as.Date(ref_date)) %>%
#   mutate(price_close = as.numeric(price_close)) %>%
#   ungroup()

```

saves
```{r, include = FALSE}
# Removes unnecessary columns and adds a 5, 10, and 20 day moving average.  These will come into play later in the code
# write.csv(df_stock_prices_ma, 'stocks0308.csv')
```

imports saved stock data
```{r,include=FALSE}
df_stock_prices_ma <- read.csv(file.choose())

df_stock_prices_ma1 <- df_stock_prices_ma %>%
  select(-c(X)) %>% 
  mutate(ref_date = as.Date(ref_date))

```

Missing Data
- Stocks are only priced on days the markets are open, so a sequence of dates has been created to account for the days when there wasn't a closing price.  
```{r}
df_date_list <- df_stock_prices_ma1 %>%
  group_by(ticker) %>%
  complete(ref_date = seq.Date(as.Date('2019-12-29'), Sys.Date(), by="day")) %>%
  ungroup()
```

Replaces NAs with 10dma.
- 5-day or 20-day moving average can be used as well
- Sales data starts on Feb 1 2020
```{r}
df_stock_prices_ma_clean <- df_date_list %>%
  group_by(ticker) %>%
  fill(mov_avg10, .direction = "downup")  %>%
  mutate(price_close = ifelse(is.na(price_close), mov_avg10, price_close))  %>%
  select(c(ticker,ref_date,price_close)) %>%
  filter(ref_date >= '2020-02-01') %>% # first day of sales data
  mutate(price_close = round(price_close,2))%>%
  ungroup()
```

5dma Stock Price data 
- Alternatively, a df is created based just on the 5-day moving average of the share price - 1 week
```{r}
df_stock_prices_5dma <- df_date_list %>%
  group_by(ticker) %>%
  fill(mov_avg5, .direction = "downup")  %>%
  select(c(ticker,ref_date,mov_avg5)) %>%
  filter(ref_date >= '2020-02-01') %>% #most recent sales data
  mutate(mov_avg5 = round(mov_avg5,2)) %>%
  select(ref_date, ticker, mov_avg5) %>%
  as.data.frame()
```

# Sales Data
Import Data
```{r,include=FALSE}
df_sales <- read.csv(file.choose())
```

Converts sales_df date to appropriate date forma and creates a 7days moving average for sales
```{r}
df_sales$Date <- as.Date(df_sales$Date, format = "%m/%d/%Y")
df_sales_7dma <- df_sales %>%
  mutate(mov_avg7 = round(SMA(Sales, n =7),2))
```

merges daily sales and the daily stocks data
```{r}
df_final_daily_daily <- merge(df_stock_prices_ma_clean, df_sales, by.x='ref_date',by.y='Date', all.x=TRUE) %>% 
  pivot_wider(names_from = ticker, values_from = price_close) %>%
  filter(Sales >=0) %>%
  as.data.frame()

```

merges the 7dma sales and the daily stocks data
```{r}
df_final_7dma_daily <- merge(df_stock_prices_ma_clean, df_sales_7dma, by.x='ref_date',by.y='Date', all.x=TRUE) %>% 
  select(-c(Sales)) %>%
  pivot_wider(names_from = ticker, values_from = price_close) %>%
  filter(mov_avg7 >=0) %>%
  as.data.frame()
```

merges the 7dma sales and 5dma stocks data
```{r}
df_final_7dma_5dma <- merge(df_stock_prices_5dma, df_sales_7dma, by.x='ref_date',by.y='Date', all.x=TRUE) %>% 
  select(-c(Sales)) %>%
  pivot_wider(names_from = ticker, values_from = mov_avg5) %>%
  filter(mov_avg7 >=0)%>%
  as.data.frame()
```

# Control Group
In order to create a synthetic control group - aka a baseline - we want to see which securities are most correlated to the number of sales.  Actual daily values are used initially, but log values will be used as well
```{r warning= FALSE}
m <- df_final_daily_daily[,-1]
a <- corr_var(m,
              Sales,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a, cex = .5,cex.axis=.5)
```
Correlations don't look too strong, the most correlated is MOH with a corr of -0.54.  Will try with log values of sales

Log values of daily sales and stock prices
```{r warning= FALSE}
m_log <- log(df_final_daily_daily[,-1])
a_log <- corr_var(m_log,
              Sales,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_log, cex = .5,cex.axis=.5)
```
These results are actually worse than before - with the most correlated being -0.347. Will try 7dma and 5dma

7dma Sales and daily stock prices
```{r}
m_7dma_daily <- df_final_7dma_daily[,-1]
a_7dma_daily <- corr_var(m_7dma_daily,
              mov_avg7,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_7dma_daily, cex = .5,cex.axis=.5)
```
These results look a lot better, ranging from -.783 to -.727.

7dma sales and 5dma stock prices
```{r}
m_7dma_5dma <- df_final_7dma_5dma[,-1]
a_7dma_5dma <- corr_var(m_7dma_5dma,
              mov_avg7,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_7dma_5dma, cex = .5,cex.axis=.5)
```
7dma and 5dma are similar to the above, ranging from -.783 to -.727, but include a positive correlation with T

Log 7dma sales and 5dma stock prices
```{r}
m_7dma_5dma_log <- log(df_final_7dma_5dma[,-1])
a_7dma_5dma_log <- corr_var(m_7dma_5dma_log,
              mov_avg7,
              top = 25,
              #max_pvalue = 0.05
              )
plot(a_7dma_5dma_log, cex = .5,cex.axis=.5)
```
These have provider the best thus far, ranging from -0.849 to -0.777

Top 100 for log 7dma and 5dma
```{r}
m_7dma_5dma_log <- log(df_final_7dma_5dma[,-1])
a_7dma_5dma_log <- corr_var(m_7dma_5dma_log,
              mov_avg7,
              top = 100,
              #max_pvalue = 0.05
              )
log_corr <- a_7dma_5dma_log$data$corr
log_var <- a_7dma_5dma_log$data$variables
log_corr_var <- data.frame(col1 = log_var,col2 =log_corr)
log_corr_var
```
All hundred are above -0.675

```{r}
model_df <- df_final_7dma_5dma
```


# Modeling

Need to establish the pre and post periods.  These periods can be shortened as we tune the model if needed. The intervention (price increase) occurred on Jan 15th 2023. A variable called time points is created for the Causal Impact model.

```{r}
pre.start <- "2020-03-01" #start of pre period
pre.end <- "2023-01-14" #end of pre period
post.start <- "2023-01-15" #intervention start
post.end <- "2023-02-27" #end of data/intervention end

pre.period <- as.Date(c(pre.start, pre.end))
post.period <- as.Date(c(post.start,post.end))

model_df_clean <- model_df %>% filter(ref_date >= pre.start) %>% select(-c(ref_date)) %>% log()

time.points <- seq.Date(as.Date(pre.start), as.Date(post.end), by =1)
```


Other variables are needed for the model  The first 50 most correlated stocks will be used initially.  The top 100 were all fairly high, so it might be worth exploring all 100, more than 100, and/or less than 100. 
```{r}
baseline_tickers <- a_7dma_5dma_log$data$variables[1:50] #takes the first 50 stocks from the correlation created earlier.

model_df_clean1 <- as_tibble(model_df_clean[,c('mov_avg7', baseline_tickers)]) # creates a tibble for the model

baseline <- zoo(model_df_clean1, order.by = time.points) #creates a time series for stocks and time.points
```

Initial run using default values
```{r}
ptm <- proc.time() #starts the clock

impact_defaults <- CausalImpact(baseline, 
                                pre.period, 
                                post.period
                                )
                                
summary(impact_defaults)

proc.time() - ptm #stops the clock


```

Results aren't significant, but increasing the iterations might produce better results

```{r}
impact_iter <- CausalImpact(baseline, 
                                pre.period, 
                                post.period,
                                model.args = list(
                                  niter = 100000
                                  # prior.level.sd = 0.01, #default is 0.01
                                  # nseasons = 1, #default is 1
                                  # season.duration = 1, #default is 1
                                  # dynamic.regression = FALSE, #default is FALSE
                                  # max.flips = -1 #default is -1
                                  )
                                )
                                
summary(impact_iter)

proc.time() - ptm #stops the clock


```
10,000 was a little better, but not a ton - p-value 0.22911
100,000 was a little better, but not a ton - p-value 0.22006

It could be that the price increase did not have an impact on overall sales, but maybe changing some of the hyperparameters will paint a better picture

# Hyperparameter tuning

Creates a grid of various hyperparameters
```{r}
param_grid = list(niter = c(1000), # default is 1000
                  standardize.data = c(TRUE, FALSE), #default is true
                  prior.level.sd = c(0.01,0.001, 0.1), # default is 0.01
                  nseasons = c(1), #default is 1
                  season.duration = c(1), #default is 1
                  dynamic.regression = c(FALSE,TRUE), #default is false
                  max.flips = c(-1,25,50) #default is -1
                 )

# Generate all combinations of parameters
params = expand.grid(param_grid)
params
```

Runs various models and stores p-value
```{r}
#creates an empty list to store p values of each model
p = c()

ptm <- proc.time() #starts the clock
for(i in 1:nrow(params)){
# for(i in 1:nrow(test_param)){

  # Fit a model using one parameter combination
  m = CausalImpact(baseline,
                   pre.period, 
                   post.period,
                   #alpha = 0.1, # can adjust alpha if necessary
                   model.args = list(
                     niter = params[i,1],
                     standardize.data = params[i,2],
                     prior.level.sd = params[i,3],
                     nseasons = params[i,4],
                     season.duration = params[i,5], 
                     dynamic.regression = params[i,6],
                     max.flips = params[i,7]
                     )
                   )

  # Model performance
  df_p <- m$summary$p[1]
  # Save model performance metrics
  p[i] = df_p
  print(proc.time() - ptm)[3]

}
 proc.time() - ptm #stops the clock
```

```{r}
results <- data.frame(params,p) %>% arrange(p)
results
```

